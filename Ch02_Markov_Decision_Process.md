# 马尔可夫决策过程

马尔可夫决策过程 (Markov Decision Process, MDP) 是强化学习中的基本问题模型之一.
- 目的是以数学形式来描述 Agent(智能体) 与 environment(环境)交互 中完成目标学习的过程. 即, 用数学形式表述一个学习过层.

智能体与环境交互的马尔可夫决策过程过程如下图所示:
![](Ch02_Markov_Decision_Making_process_images/智能体与环境交互的过程.png)
- 图中 t 表示 time step, $t = 1, 2, 3,...$
  > t 与现实时间并不是一一对应的关系, 它描述的是 Agent 进行交互并获得反馈所需要的时间.
- 每个 time step 中, Agent 从 Environment 中观察到一个 state(状态) $s_t$, 并根据当前状态 $s_t$ 和当前奖励 $r_{t}$ 选择并执行一个动作 Action $a_t$.

- 执行动作 $a_t$ 后, Agent 会得到一个 reward(奖励) $r_{t+1}$, 在执行动作 $a_t$ 的过程中, Environment 也会受到影响并进入到下一个状态 $s_{t+1}$.
    > - 我们将$a_t$的奖励记为 $r_{t+1}$ 而不是 $r_t$ 是为了强调获得奖励的时候 time step 已经更新到 t+1 了
    > - Agent 得到的奖励 $r_t$ 在实际情况中可能是一个**随机变量或函数** 而不是一个定值, 有些书籍会将它记为 $R_t$, 这里为了便于概率论相关公式的推导, 暂时用 $r_t$.

此时, 根据这个图, 我们会记录到一系列轨迹:
$$
s_0, a_0, r_1, s_1, a_1, r_2, ..., s_t, a_t, r_{t+1}, ...
$$

我这里只探讨有限马尔可夫决策过程 (Finite MDP), 因此 t 是有限的, 上限为 T.
- 通常, $[0, T]$ 这个时间范围被称为一个 `episode`(回合). 例如游戏中的一局.
  - T 也是最后一个 time step.



---

## 马尔可夫性质
马尔科夫性质指的是: `当前时刻的状态 只由 前一个时刻状态决定, 与 更早的历史状态 无关`. 用公式表达为:

$$
P(s_{t+1} | s_t, s_{t-1}, ..., s_0) = P(s_{t+1} | s_t)
$$

对于马尔科夫性质:
- **允许**我们在没有考虑完整的历史经历的情况下, 近依靠最近的一次状态进行预测和控制 Agent 的行为.
- 实际问题中有很多**不满足**马尔科夫性质的例子, 比如棋类游戏, 它需要考虑当前状态以及所有历史状态.
    - 此时我们可以用深度学习神经网络来表示当前的棋局，并用`蒙特卡洛搜索树`等技术来模拟玩家的策略和未来可能的状态，来构建一个新的决策模型，这就是著名的 [AlphaGo](https://www.nature.com/articles/nature16961) 算法.


--- 

## 回报 (Return)
最开始我们有提到, 有限马尔可夫决策过程的实现目标是在有限的 time step 前提下`让 Agent 的累积奖励最大化`. 这里为了描述方便, 将**累积奖励**记为`回报 (Return)`, 用 $G_t$ 表示, 写成数学公式如下:
$$
G_t = r_{t+1} + r_{t+2} + ... + r_T
$$
- 此公式仅适用于 `T为有限值` 的场景, 比如玩游戏时, 我们总是在某个步数 (即T) 后以特殊的状态结束游戏.

对于**无限马尔可夫决策过程**, 我们需要引入一个折扣因子 $\gamma$ 来调整回报的计算方式:
$$
G_t = r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + ... = \sum_{k=0}^{\infty} \gamma^k r_{t+k+1}
$$
- 其中, $\gamma$ 是一个介于 0 和 1 之间的值, 用来调整未来奖励的重要性.
  - 当 $\gamma = 0$ 时, Agent **只关注当前的奖励**, 不考虑未来奖励.
  - 当 $\gamma = 1$ 时, Agent 会**着重考虑所有未来奖励**.

引入 $\gamma$ 还可以将当前 time step 的回报与未来 time step 的回报进行关联, 从而更好地指导 Agent 的行为. 也就是说, 我们可以获得下面的递推式:

$$
\begin{aligned}
G_t &= r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \ldots \\
    &= r_{t+1} + \gamma (r_{t+2} + \gamma r_{t+3} + \ldots) \\
    &= r_{t+1} + \gamma G_{t+1}
\end{aligned}
$$

这个公式对于所有 $t<T$ 都成立, 并且在 **贝尔曼公式** 中这个推导式尤其重要.

---



