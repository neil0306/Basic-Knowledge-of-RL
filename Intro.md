# 什么是强化学习

强化学习是一种机器学习方法，旨在让智能体学会在一个环境中采取行动以达到某种目标。它**通过与环境互动来学习最优的行为策略**，即通过试错和奖励来优化决策过程。强化学习通常涉及以下要素：

1. **智能体（Agent）**：执行动作并学习的实体，可以是机器人、程序等。
2. **环境（Environment）**：智能体所处的外部环境，对智能体的行动和状态产生影响。
3. **行动（Action）**：智能体可以采取的操作或决策，影响环境的变化。
4. **状态（State）**：描述环境的特定情况或状态，对智能体的行动和奖励产生影响。
5. **奖励（Reward）**：智能体根据其行动获得的反馈，用于评估行为的好坏。
6. **策略（Policy）**：智能体的决策规则，用于选择行动以最大化长期奖励。

在强化学习中，智能体通过尝试不同的行动，并根据奖励信号来调整策略，逐步优化其行为。这种学习方式特别适用于需要长期决策和试错学习的问题，例如自动驾驶、游戏策略优化等领域。

**强化学习是目前最高效的学习方法之一.**

## 试错学习
`试错学习(trial and error learning)` 是强化学习最鲜明的要素之一,  它一般包含以下4个特征:
1. 尝试: 首先采取某种行动, 哪怕这个行动是随机行为, 比如模型权重的初始化
2. 错误: 在进行尝试的过程中, 我们总会遇到错误, 这种错误的产生有多种可能, 比如模型的输出结果与 ground truth 之间的差异, 比如环境(如噪声)造成的不确定导致结果与预期存在偏差.
3. 结果: 每次尝试的后果, 无论是积极或者消极, 这个结果都会对下一次的尝试产生直接或间接影响.
4. 学习: 通过多次尝试, 根据结果修正自身行为, 这就是学习的过程.

强化学习除了上述的试错学习之外, 还包含其他的学习形式, 比如`观察学习/模仿学习`等.


## 强化学习里的一些概念

决策 (decision):
- 每一次尝试行为就是一种决策.
- 决策带来的后果可能是即时的, 也可能是长期的(需要等待一段时间后才会触发)

奖励 (reward):
- 尝试所产生的好的结果被称为`奖励`

惩罚 (punishment):
- 尝试所产生的坏的结果被称为`惩罚`
- 惩罚有时也被成为`负的奖励`

序列决策 (sequential decision making):
- 我们通常将通过**多次决策**实现**累积奖励最大化**的过程称为`序列决策` 
- 对于任意问题, 只要能见磨成序列决策问题, 或者带有鲜明的试错学习特征就可以使用强化学习来解决.

---

# 强化学习的应用
游戏领域:
- AlphaGo 是围棋AI的代表作, 通过强化学习的方法, 在围棋领域取得了巨大的成功.
- AlphaZero 是一个通用游戏AI.
- AlphaStar 在星际争霸2游戏中取得了巨大的成功.

机器人领域:
- 在机器人中实现强化学习的成本往往较高，一方面**观测环境的状态需要大量的传感器**，另一方面则是**试错学习带来的实验成本**，在训练过程中如果机器人决策稍有失误就有**可能导致设备损坏**，因此在实际应用中往往需要结合其他的方法来辅助强化学习进行决策.
  - 通常会使用仿真环境进行模拟. Visual Language Navigation (VLN) 就是一个典型的例子.
- NICO 机器人学习抓取物体就是强化学习在机器人中应用的经典例子.

领域很多, 不再一一列举.

---

# 强化学习研究方向
多智能体强化学习 (multi-agent reinforcement learning, MARL):
- 意思是在多个智能体的环境下进行强化学习.
- 存在的问题:
  - 多个智能体之间如何高效通信.
  - 如何协作, 协作时的贡献或责任怎么界定.
  - 复杂博弈场景怎么进行均衡.


从数据中学习, 或者从演示中学习 (learn from demostration):
- 在模仿学习中, 如何通过从专家数据中学习策略
  - 模仿学习是指奖励函数难以明确定义或者策略本身就很难学出来的情况下，我们可以通过模仿人类的行为来学习到一个较好的策略.
    - 经典模仿策略之一就是`行为克隆(behavior cloning, BC)`, 通过监督学习的方法(比如训练神经网络)来学习策略. 这种方法的缺点是容易受到 数据分布漂移(data distribution shift) 的影响. 此时智能体会遇到从未见过的状态, 导致策略失效.
- 在逆强化学习 (inverse reinforcement learning, IRL) 中, 如何从人类数据中**学习奖励函数**.
  - 逆强化学习会受到噪声的影响，因此如何从噪声数据中学习到一个较好的奖励函数也是一个难题。
-  RLHF (Reinforcement Learning from Human Feedback) 如何从人类的反馈中学习**奖励模型**来进行微调 (fine-tune).
- 离线强化学习 (offline reinforcement learning) 如何从历史数据中学习策略.
- 世界模型 (world model)
  - 在离线环境中训练一个世界模型，然后将世界模型部署到在线环境中进行决策。世界模型的思路是将环境分为两个部分, 一个是世界模型，另一个是控制器。世界模型的作用是预测下一个状态，而控制器的作用是根据当前的状态来决策动作。
  - 难题: 
    - 世界模型的预测误差会导致控制器的决策出错，因此如何提高世界模型的预测精度也是一个难题。



探索策略 (exploration strategy):
- 如何在探索和利用之间取得平衡. 
  - 在探索的过程中，智能体会尝试一些未知的动作，从而可能会获得更多的奖励，但同时也可能会遭受到惩罚。而在利用的过程中，智能体会选择已知的动作，从而可能会获得较少的奖励，但同时也可能会遭受较少的惩罚.
  - 比较常用的方法有 `ε-greedy`, `UCB (upper confidence bound)`, `Thompson Sampling` 等.
  - 提高探索的本质也是为了**避免局部最优**问题，从而提高智能体的鲁棒性，近年来也有研究结合进化算法来提高探索的效率, 如 `NEAT (neuro evolution of augmenting topologies)`, `PBT(population-based training)` 等.
    - 这些算法增加计算成本.


实时环境 (real-time environment):
- 一种是研究世界模型
- 一种是研究离线环境下的强化学习


多任务强化学习 (multi-task reinforcement learning):
- 智能体往往需要同时解决多个任务，例如机器人需要同时完成抓取、搬运、放置等任务，而不是单一的抓取任务。在这种情况下，如何在多个任务之间做出权衡是一个难题。
  - 常见的训练方法是 `联合训练 (join training)` 和 `分层强化学习 (hierarchical reinforcement learning)`
    - 联合训练的思路是**将多个任务的奖励进行加权求和**，然后通过强化学习来学习一个策略。
    - 分层强化学习的思路是将多个任务分为两个层次，一个是**高层策略**，另一个是**低层策略**。高层策略的作用是决策当前的任务，而低层策略的作用是决策当前任务的动作。这样就可以通过强化学习来学习高层策略和低层策略，从而解决多任务强化学习的问题。
      - 存在问题: 
        - 高层策略的决策可能会导致低层策略的决策出错，因此如何提高高层策略的决策精度也是一个难题.



