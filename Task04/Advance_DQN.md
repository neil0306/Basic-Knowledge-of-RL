# Overview
这里主要提到一些 DQN 的改进算法, 包括 Double DQN, Dueling DQN, Noisy DQN, PER DQN算法等. 他们的改进主要目的还是通过**提高预测的精度和控制过程中的探索度**来改善DQN的性能. 主要改进方向有:
- 改进网络模型
- 改进经验回放

# Double DQN 算法
Double DQN 算法是目前较为成熟的 Nature DQN 算法, 它的主要贡献在于引入了两个网络来解决Q值过估计(overestimation)的问题. 为了避免与 DQN 中提到的 "目标网络" 混淆, 这里简单梳理一下发表历史:
- 2013年 DQN 发表, 它只是在 Q-learning 基础上引入了神经网络而已, 没有其他额外的技巧
- 在2013~2015年间, Double DQN 算法被提出, 然后 DeepMind 公司在 2015年借鉴了 Double DQN, 在 nature 上发表了新的DQN算法, 称为 Nature DQN.

因为 Double DQN 这个改进网络比较经典, 所以单独拿出来讲讲.

## Q值过估计
首先回顾一下DQN的公式:
$$
Q_{\theta}(s_t, a_t) \leftarrow Q_{\theta}(s_t, a_t) + \alpha \left[ r_t + \gamma \max_a Q_{\hat{\theta}}(s_{t+1}, a) - Q_{\theta}(s_t, a_t) \right]
$$
- 其中 $y_t = r_t + \gamma \max_a Q_{\hat{\theta}}(s_{t+1}, a)$ 表示Q的估计值, $\hat{\theta}$ 表示**目标网络**的参数.
- 由于这里直接拿目标网络中各个动作对应的**最大的 Q 值来当作估计值**，这样一来**就会存在过估计的问题**。

Double DQN 为了处理这个Q值过估计问题, 使用方法是:
```txt
现在"当前网络"中找出最大 Q 值对应的动作，然后再将这个动作代入到"目标网络"中去计算 Q 值.
```
即:
1. 先用 "当前网络" (每处理一个样本就更新一下参数) 找出最大 Q 值所对应的 动作$\alpha$
$$
\alpha^{max}_{\theta}(s_{t+1}) = \arg \max_a Q_{\theta}(s_{t+1}, a)
$$
2. 将这个 动作 代入到 "目标网络" (更新没那么频繁的网络, 并且是用于计算 $y_i$ 的那个网络) 中计算目标的 Q 值, 从而得到 估值$y_i$.
$$
y_t = r_t + \gamma \max_a Q_{\hat{\theta}}(s_{t+1}, \alpha^{max}_{\theta}(s_{t+1}))
$$

这个过程相当于把 动作选择 和 动作评估 分离, 从而减轻了过估计问题.
- 动作选择的过程相当于 去收集情报, 
- 动作评估则是 进行一定程度的筛选, 最后"上报"给模型的是相对优质的结果, 此时模型参数的更新会显得更有意义.

相比 Nature DQN, 它没有经过筛选的过程, 而是直接直接"上报", 不管三七二十一直接更新参数.

对于训练过程:
- DQN (或者说 Nature DQN) 是每隔 C 次迭代就复制一次"当前网络"的参数给"目标网络"
- 而 Double DQN 则是每次迭代都会随机从这两个网络中选一个进行更新
  - 这时候, "目标网络" 和 "当前网络" 的就不是那么固定了, 参数被更新的那个网络会充当"当前网络", 而另一个没有更新参数的网络就是"目标网络", 它负责计算 $y_t$.

由于 Nature DQN 那种直接复制参数的方便简单粗暴, 容易实现, 所以一般都采用这种直接复制的办法.

---

# Dueling DQN 算法
这个算法主要针对神经网络的结构进行优化.
在DQN中, 它的网络很简单, 就一个 MLP:
![](advance_DQN_images/DQN网络结构.png)

在 Dueling DQN 中, 它只是用了两个分支而已:
![](advance_DQN_images/Dueling_DQN网络结构.png)
- 优势层(advanture layer): 估计每个动作带来的优势
- 价值层(Value layer): 估计每个状态的价值, 输出维度是1

用公式表示为:
$$
Q_{\theta, \alpha, \beta}(s,a) = (A_{\theta,\alpha}(s,a) - \frac{1}{|A|} \sum_{a \in \Alpha} A_{\theta,\alpha}(s,a)) + V_{\theta,\beta}(s) 
$$
- 其中 $A_{\theta,\alpha}(s,a)$ 表示优势层, $\theta$表示共享的隐藏层参数, $\alpha$ 表示优势层自己这部分参数
- 相应地, $V_{\theta,\beta}(s)$ 表示价值层, $\theta$表示共享的隐藏层参数, $\beta$ 表示价值层自己这部分参数

后面的章节中会提到一个很相似的网络, 叫 Actor-Critic 网络, 它也是有两个分支结构, 优势层相当于 actor, 价值层相当于 critic. 不一样的是, Actor和Critic是分开的两个独立网络, 而 Dueling DQN 是在一个网络中实现的.
- 这种分支结构有助于隔离每个网络输出上的影响，并且只更新适当的子网络，这有助于**降低方差并提高学习鲁棒性**。

---

# Noisy DQN 算法
这个网络也是改进网络结构, 不过它的目标不是提高Q值的估计, 而是增强网络的探索能力.

Noisy DQN 网络在 DQN 的基础上引入了噪声层, 即将随机性应用到网络参数中, 增加了网络对状态和动作空间探索的能力. 从而提高收敛速度和稳定性. 
- 实现的时候就简单地给线性层加上噪声即可.

注:
- 给网络权重加噪声在强化学习中是一个很常用的trick.


---

# PER (Prioritized experience replay) DQN 算法
PER (Prioritized experience replay, 优先经验回放) DQN 是对经验回放的改进, 它在采样过程中**赋予经验回放中样本的优先级**.
- 优先级的赋予依据是 Time Different (TD, 时序差分). 
  - 我们每次从经验回访中取出一个批量的样本，进而计算的 TD 误差一般是不同的，对于 DQN 网络反向传播的作用也是不同的。TD误差越大，损失函数的值也越大，对于反向传播的作用也就越大。 这样一来如果 TD 误差较大的样本更容易被采到的话，那么我们的算法也会更加容易收敛。因此我们只需要设计一个经验回放，根据经验回放中的每个样本计算出的TD误差赋予对应的优先级，然后在采样的时候取出优先级较大的样本。

实现的时候则是借助 SumTree 这样的二叉树结构来实现:
![](advance_DQN_images/SumTree.png)
- 最底层是样本和对应的优先级, 图中每个节点的值就是他们的 TD error.
- 并且根据叶子节点的值，我们从 0 开始依次划分采样区间。然后在采样中，例如这里根节点值为 66 ，那么我们就可以在 [0,66) 这个区间均匀采样，采样到的值落在哪个区间中，就说明对应的样本就是我们要采样的样本。例如我们采样到了 25 这个值，即对应区间 [0,31)，那么我们就采样到了第一个叶子节点对应的样本。
- 第一个样本对应的区间也是最长的，这意味着第一个样本的优先级最高，也就是 TD 误差最大，反之第四个样本的区间最短，优先级也最低。这样一来，我们就可以通过采样来实现优先经验回放的功能。


在实现时, 考虑效率因素, 每次更新 SumTree 的时候并不会把经验回放中的所有样本都计算 TD error 和 相应的优先级. 而是只更新当前取到的小批量样本, 此时, 每次计算 TD error 对应的其实是之前的网络, 而非当前 time step 下的网络.
- 这时候, 又引入了新问题:
  1. 如果某批量样本的 TD 误差较低，只能说明它们对于之前的网络来说“信息量”不大，但不能说明对当前的网络“信息量”不大，因此单纯根据 TD 误差进行优先采样有可能会错过对当前网络“信息量”更大的样本。
  2. 其次，被选中样本的 TD 误差会在当前更新后下降，然后优先级会排到后面去，下次这些样本就不会被选中，这样来来回回都是那几个样本，很容易出现“旱的旱死，涝的涝死”的情况，导致过拟合。

解决方案是引入随机优先级采样(stochastic prioritization)和重要性采样(importance sampling)的方法:

---

## 随机优先级采样
随机优先级采样指的是在每次更新时, 不是直接采样 TD error 最大的样本, 而是根据下面式子定义好的概率进行采样:
$$
P(i) = \frac{p_i^{\alpha}}{\sum_k p_k^{\alpha}}
$$
- 式中 $p_i$ 是样本 i 的优先级; 
- $\alpha$ 是一个超参数, 用来控制采样的随机性, 取值范围在$[0,1]$区间中.当$\alpha = 0$时, 采样变成均匀分布上的采样; 当$\alpha = 1$时, 采样变成线性分布, 此时称为优先级采样.

为了使最低优先级的分布的采样概率不为0, 通常会增加一个很小的值 $\epsilon$:
$$
p_i = |\delta_i| + \epsilon 
$$
- 其中 $\delta_i$ 是 TD error, $\epsilon$ 是一个很小的值, 用来保证每个样本都有被采样的机会.

除了使用幂次来设置优先级之外, 也可以用排名的方式决定优先级:
$$
p_i = \frac{1}{rank(i)}
$$
- 这种方式也能保证采样概率不为零, 不过一般也会增加一个很小的值 $\epsilon$.


## 重要性采样 (重点! PPO算法会用!)
这是一种`估计某一分布性质`的方法, 基本思想是:
- 通过在另一个分布中的采样(这个分布一定不同于待估计的分布), 然后通过 **采样的样本权重** 来估计待估计分布的性质.

数学公式如下:
$$
\begin{aligned}
  \mathbb{E}_{x \sim p}[f(x)] &= \int f(x) p(x) dx  \\
          &= \int f(x) \frac{p(x)}{q(x)} q(x) dx \\ 
          & = \int f(x) \frac{p(x)}{q(x)} \frac{q(x)}{p(x)} p(x) dx \\
          &= \mathbb{E}_{x \sim q} \left[ \frac{p(x)}{q(x)} f(x) \right]
\end{aligned}
$$
- 其中 $q(x)$ 是采样分布, $p(x)$ 是待估计的分布, $f(x)$ 是待估计分布的性质.

接着前面的分析, 我们每次计算 TD error 对应的是之前的网络, 而不是待更新的网络, 因此, 我们实际上就获得了之前网络中的一批样本, 即 $q(x)$ 此时是已知的, 现在, 我们只需要知道 之前网络与当前待估计的网络分布之间的权重 $\frac{p(x)}{q(x)}$, 就能得知当前网络具有的性质 $f(x)$. 为了简化表达, 这里用 $w_i$ 定义权重:

$$
w_i = \left( \frac{1}{N} \cdot \frac{1}{P(i)} \right)^{\beta}
$$
- N 是经验回放中的样本总数, $P(i)$ 是样本 i 的采样概率.

为了避免权重过大, 一般会对权重进行归一化处理:
$$
w_i = \frac{(N * P(i))^\beta}{\max_j (w_j)}
$$
- 上面两个式子中, $\beta$ 都是是一个超参数, 用来控制重要性采样的程度, 一般取值在$[0,1]$区间中. 
  - 当$\beta = 0$时, 重要性采样不起作用; 
  - 当$\beta = 1$时, 完全考虑重要性采样.
> 实际使用中, 我们希望 $\beta$ 随着训练的进行逐渐增大, 从而使得网络更加关注那些 TD error 较大的样本.

这个思想也称为 `热偏执(annealing the bias)`, 也是 PPO 算法中的一个重要技巧. 用数学方式表示为:
$$
\beta = \min(1, \beta + \beta_{step})
$$
- $\beta_{step}$ 是每个 time step 对应的增加值. 实际使用中, 一般设置为很小的数值, 比如 $0.0001$.

这种做法就可以在训练的前期先用 随机优先级采样, 然后在训练的后期逐渐引入重要性采样, 从而使得网络更加关注那些 TD error 较大的样本.


# C51 
C51 算法也称为`Distributed DQN`或`Categorical DQN`. 它的目的是能适用于任何基于 Q-learning 的强化学习算法.
- 论文: [A Distributional Perspective on Reinforcement Learning](https://arxiv.org/abs/1707.06887)

C51算法核心思想是将DQN算法中的**值函数** `Q(s,a)` 换成 **值分布函数** `Z(s,a)`, 也就是将输出从一个值, 换成了一个分布.
- 优点是可能更好地处理`值函数估计不准确`以及`离散动作空间`的问题.
- 在之前讲到的经典强化学习算法中我们优化的其实是值分布的均值，也就是 Q 函数，但实际上由于状态转移的随机性、函数近似等原因，智能体与环境之间也存在着随机性，这也导致了**最终累积的回报也会是一个随机变量**，**使用一个确定的均值会忽略整个分布所提供的信息**。

基于这个出发点, 我们将Q当做随机变量, Q的**期望就是原来定义的值函数**, Q的方差则是Q函数的不确定性:
$$
\begin{aligned}
  Q^{\pi}(x,a) &= \mathbb{E} Z^{\pi}(x,a) \\
  &= \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R(x_t, a_t) \right]
\end{aligned}
$$

- 状态分布 $x_t \sim P(\cdot |x_{t-1}, a_{t-1}), a_t \sim \pi(\cdot | x_t)$, $x_0 = x, a_0 = a$













