# PPO简介
PPO是典型的 `Actor-Critic` 算法, 同时适用于连续和离散的动作空间.

PPO 在2017由 OpenAI 的 Schulman 等人提出. 核心思想是通过`在策略梯度的优化过程中引入重要性权重来限制策略更新的幅度`, 从而提高算法的稳定性和收敛性. 
- 优点是简单, 易于实现, 调参简单. (遇事不决PPO)

PPO 的前身是 TRPO算法. TRPO算法也是基于策略梯度, 核心思想是 "定义策略更新的信赖域(trust region), 以此保证每次更新的策略与当前的策略偏差不会太大, 从而避免更新后性能下降". 但 TRPO 算法的缺点是约束优化问题过于复杂, 计算繁琐, 难以实现.
- 这里不详细展开, 原文可以参考: https://arxiv.org/abs/1502.05477


---

# 重要性采样
重要性采样(importance sampling) 是 PPO 里用到的非常重要的操作. 从概率论来说, 它是`一种估计 "随机变量的期望" 或者 "随机变量的概率分布" 的统计方法`.

简单描述一下重要性采样是个什么东西:
- 场景: 
    > 假设我们有一个函数$f(x)$, 计算$f(x)$的期望需要依赖 **目标分布** $p(x)$ 的采样值, 但是我们不仅知道$p(x)$的具体形式, 并且想要从中采样也极为困难.

- 核心思想: 
    > 来一波狸猫换太子. 
    > 找个好操作的分布 $q(x)$, 只要我们从中采样的样本都可以 "修正" 到目标分布$p(x)$上, 那么我们就可以通过这些修正过的样本来近似得到 $f(x)$ 的期望值.
    >> 这个 "修正" 的工具就是所谓的 "重要性权重".
- 具体做法:
    > 找到一个**提议分布(proposal distribution)** $q(x)$, 从这个分布采样是一件很简单的事情, 对于每一个采样的样本, 乘以一个重要性权重 $\frac{p(x)}{q(x)}$, 就可以得到一个 "修正" 后的样本, 从而计算出 $f(x)$ 的期望.

用数学公式描述这个操作:
- 如果x是连续随机变量:
$$
\begin{aligned}
\mathbb{E}_{p(x)}[f(x)] &= \int f(x) p(x)  dx \\
&= \int f(x) q(x) \frac{p(x)}{q(x)}   dx \\
&= \mathbb{E}_{q(x)}\left[f(x) \frac{p(x)}{q(x)} \right]
\end{aligned}
$$

- 如果x是离散随机变量:
$$
\begin{aligned}
\mathbb{E}_{p(x)}[f(x)] &= \frac{1}{N} \sum f(x_i) p(x_i) \\
&= \frac{1}{N} \sum f(x_i) q(x_i) \frac{p(x_i)}{q(x_i)} \\
&= \frac{1}{N} \sum f(x_i) \frac{p(x_i)}{q(x_i)}
\end{aligned}
$$

这时候, 本来看着很难的事情就看起来简单多了, 不过此时还需注意: 
- 当$p(x)$不为0时, $q(x)$ 也不能为零, 但是他们可以同时为零, 这时候$\frac{p(x)}{q(x)}$ 依然有定义, 具体原理这里先不管, 反正先用起来.


在选取 $q(x)$ 的时候, 我们通常希望 $q(x)$ 能够尽可能的 "接近" $p(x)$, 这样重要性权重 $\frac{p(x)}{q(x)}$ 就会尽可能的接近1, **从而减小估计的方差**. 理由如下:
- 回顾方差公式:
$$
\begin{aligned}
Var_{x \sim p} [f(x)] &= \mathbb{E}_{p(x)}[(f(x) - \mathbb{E}_{p(x)}[f(x)])^2] \\
&= \mathbb{E}_{p(x)}[f(x)^2] - \mathbb{E}_{p(x)}[f(x)]^2
\end{aligned}
$$
- 重要性采样的估计方差:
$$
\begin{aligned}
Var_{x \sim q} [f(x) \frac{p(x)}{q(x)}] &= \mathbb{E}_{q(x)}[(f(x) \frac{p(x)}{q(x)} - \mathbb{E}_{q(x)}[f(x) \frac{p(x)}{q(x)} )^2] \\
&= \mathbb{E}_{q(x)}[f(x)^2 \frac{p(x)^2}{q(x)^2}] - \mathbb{E}_{q(x)}[f(x) \frac{p(x)}{q(x)}]^2 \\ 
&= \mathbb{E}_{p(x)}[f(x)^2 \frac{p(x)}{q(x)}] - \mathbb{E}_{p(x)}[f(x)]^2
\end{aligned}
$$
  - 公式里的最后一行是因为 $\mathbb{E}_{q(x)}[f(x) \frac{p(x)}{q(x)}] = \mathbb{E}_{p(x)}[f(x)]$. 替换分布的时候, 里面的随机变量 $f(x)\frac{p(x)}{q(x)}$也一起给换成$f(x)$了.
  - 所以说, 当 $q(x)$ 越接近 $p(x)$ 时, 重要性采样的估计方差就会越小.

其实重要性采样也是蒙特卡洛估计的一部分，只不过它是一种比较特殊的蒙特卡洛估计，允许我们在复杂问题中利用已知的简单分布进行采样，从而避免了直接采样困难分布的问题，同时通过适当的权重调整，可以使得蒙特卡洛估计更接近真实结果。






